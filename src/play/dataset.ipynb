{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5645936c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 13509\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1502\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_dir = \"D:/dolly15k/processed\"  # 事前にデータセットを保存したディレクトリを指定\n",
    "processed_dataset = load_from_disk(dataset_dir)\n",
    "\n",
    "processed_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1003a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, input_ids length: 209\n",
      "Sample 1, input_ids length: 29\n",
      "Sample 2, input_ids length: 30\n",
      "Sample 3, input_ids length: 101\n",
      "Sample 4, input_ids length: 65\n"
     ]
    }
   ],
   "source": [
    "train_dataset = processed_dataset['train']\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i}, input_ids length: {len(train_dataset[i]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e63a0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 112]) torch.Size([2, 112])\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# .envファイルの読み込み\n",
    "dotenv.load_dotenv(\".env\")\n",
    " \n",
    "# トークナイザの読み込み\n",
    "token = os.environ.get(\"HF_TOKEN\")\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, use_fast=True, token=token\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# データコラレータの定義\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer, \n",
    "#     mlm=False,\n",
    "#     pad_to_multiple_of=8\n",
    "# )\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    pad_to_multiple_of=8,\n",
    "    label_pad_token_id=-100\n",
    ")\n",
    "\n",
    "# データローダの定義\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:    \n",
    "    x = batch['input_ids']\n",
    "    y = batch['labels']\n",
    "    \n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a6dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
